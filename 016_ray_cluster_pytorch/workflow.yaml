permissions:
  - '*'
sessions:
  dashboard:
    redirect: false
jobs:
  auth:
    steps:
      - name: Authenticate kubectl
        if: ${{inputs.deploy_ray_cluster == true }}
        run: pw kube auth ${{ inputs.k8s.cluster }}
  main:
    needs:
      - auth
    steps:
      - name: Install Helm
        if: ${{inputs.deploy_ray_cluster == true }}
        run: |
          set -ex
          if ! which helm >/dev/null 2>&1; then
              echo "Helm is not installed. Downloading..."
              curl -LO https://get.helm.sh/helm-v3.14.2-linux-amd64.tar.gz
              tar -zxvf helm-v3.14.2-linux-amd64.tar.gz
              echo "helm_cmd=./linux-amd64/helm" | tee -a $OUTPUTS
          else
              echo "Helm is already installed."
              echo "helm_cmd=$(which helm)" | tee -a $OUTPUTS
          fi
        cleanup: |
          rm -r linux-amd64
          rm *.tar.gz
      - name: Install KubeRay Operator
        if: ${{inputs.deploy_ray_cluster == true }}
        env:
          helm_cmd: ${{ needs.main.outputs.helm_cmd }}
        run: |
          set -ex
          # Check if the repo is already added
          if ! ${helm_cmd} repo list | grep -q '^kuberay'; then
            echo "Adding kuberay Helm repo..."
            ${helm_cmd} repo add kuberay https://ray-project.github.io/kuberay-helm/
          else
            echo "Helm repo 'kuberay' already exists, skipping..."
          fi

          # Always update repos
          ${helm_cmd} repo update

          # Check if the release is already installed
          if ! ${helm_cmd} list -n ${{ inputs.k8s.namespace }} | grep -q '^kuberay-operator'; then
            echo "Installing kuberay-operator..."
            ${helm_cmd} install kuberay-operator kuberay/kuberay-operator \
              --version ${{ inputs.k8s.kuberay_version }} \
              -n ${{ inputs.k8s.namespace }}

            # Wait for the kuberay-operator deployment to be ready
            echo "Waiting for kuberay-operator to be ready..."
            kubectl wait --for=condition=Available --timeout=300s deployment/kuberay-operator -n ${{ inputs.k8s.namespace }}
          else
            echo "Helm release 'kuberay-operator' already installed in namespace '${{ inputs.k8s.namespace }}', skipping..."
          fi        
        cleanup: ${helm_cmd} uninstall kuberay-operator -n ${{ inputs.k8s.namespace }}
      - name: Create Ray Cluster
        if: ${{inputs.deploy_ray_cluster == true }}
        run: |
          set -ex
          cp 016_ray_cluster_pytorch/* .
          cluster_name=$(cat ray-cluster.gpu.yaml | grep name | head -n1 | awk '{ print $2 }')
          if [ -z "${cluster_name}" ]; then
            echo "ERROR: Unable to obtain Ray cluster's name."
            exit 1
          fi
          echo "cluster_name=${cluster_name}" | tee -a $OUTPUTS

          if kubectl get runtimeclass nvidia &>/dev/null; then
            echo "nvidia RuntimeClass is available"
            awk '
            /^\s*workerGroupSpecs:/ { in_worker=1 }
            in_worker && /^\s*spec:/ {
              print $0
              print "        runtimeClassName: nvidia"
              in_worker=0
              next
            }
            { print }
            ' ray-cluster.gpu.yaml > temp.yaml && mv temp.yaml ray-cluster.gpu.yaml
          fi
          sed -i "s|rayproject/ray-ml:2.2.0-gpu|${{ inputs.k8s.ray_image }}|g" ray-cluster.gpu.yaml
          kubectl apply -f ray-cluster.gpu.yaml -n ${{ inputs.k8s.namespace }}
        cleanup: kubectl delete -f ray-cluster.gpu.yaml -n ${{ inputs.k8s.namespace }}
      - name: Wait for head pod
        env:
          cluster_name: ${{ needs.main.outputs.cluster_name }}
        if: ${{inputs.deploy_ray_cluster == true }}
        run: |
          set -ex
          sleep 5
          log() {
            while true; do
              echo; echo
              kubectl describe raycluster "${cluster_name}" -n ${{ inputs.k8s.namespace }}
              sleep 60
            done
          }
          log &
          log_pid=$!
          trap "kill ${log_pid}" EXIT
          kubectl wait pod \
            -n "${{ inputs.k8s.namespace }}" \
            -l ray.io/node-type=head \
            --for=condition=Ready \
            --timeout=10m

          head_pod_name=$(kubectl get pods -n alvaro-k8s-testing | awk '{print $1}' | grep "${cluster_name}"-head)
          if [ -z "${head_pod_name}" ]; then
            echo "ERROR: Unable to obtain name of the head pod."
            exit 1
          fi
          echo "head_pod_name=${head_pod_name}" | tee -a $OUTPUTS
      - name: Select local dashboard port
        if: ${{inputs.deploy_ray_cluster == true }}
        run: |
          # Select available port
          dashboard_port=$(pw agent open-port)
          echo "dashboard_port=${dashboard_port}" | tee -a $OUTPUTS
      - name: Create session for dashboard
        if: ${{inputs.deploy_ray_cluster == true }}
        uses: parallelworks/update-session
        with:
          remotePort: '8265'
          localPort: ${{ needs.main.outputs.dashboard_port }}
          name: ${{ sessions.dashboard }}
          targetInfo:
            name: ${{ inputs.k8s.cluster }}
            namespace: ${{ inputs.k8s.namespace }}
            resourceType: pods
            resourceName: ${{ needs.main.outputs.head_pod_name }}
      - name: Install ray
        run: |
          source /pw/.miniconda3/bin/activate
          pip install --user -U "ray[default]"
      - name: Test cluster
        run: |
          # Wait for session to connect
          sleep 60
          source /pw/.miniconda3/bin/activate
          export PATH=$HOME/.local/bin:$PATH
          ray job submit --address "http://localhost:${{ needs.main.outputs.dashboard_port }}" -- python -c "import ray; ray.init(); print(ray.cluster_resources())"
      - name: Run PyTorch GPU Example
        run: |
          set -ex
          # Prevent transfer of these files to pod
          echo helm* > .rayignore
          echo logs/ >> .rayignore
          echo linux-amd64/ >> .rayignore
          
          # Submit the training job to your ray cluster
          source /pw/.miniconda3/bin/activate
          export PATH=$HOME/.local/bin:$PATH
          sed -i "s|8265|${{ needs.main.outputs.dashboard_port }}|g" submit_job.py
          job_id=$(python submit_job.py)
          echo "Submitted Ray job with ID: $job_id"
          ray job logs $job_id --address "http://127.0.0.1:${{ needs.main.outputs.dashboard_port }}" --follow

      - name: Wait for cancel
        if: ${{inputs.deploy_ray_cluster == true }}
        run: sleep inf
'on':
  execute:
    inputs:
      deploy_ray_cluster:
        type: boolean
        default: true
        tooltip: Select Yes to deploy the service and No to enter a port for an existing service
      service_port:
        label: Service Port
        type: number
        hidden: ${{inputs.deploy_ray_cluster == true }}
        ignore: ${{ .hidden }}
      k8s:
        type: group
        label: Kubernetes
        hidden: ${{inputs.deploy_ray_cluster == false }}
        items:
          cluster:
            label: Kubernetes cluster
            type: kubernetes-clusters
            hidden: ${{inputs.deploy_ray_cluster == false }}
            ignore: ${{ .hidden }}
          namespace:
            label: Namespace
            type: kubernetes-namespaces
            clusterName: ${{ inputs.k8s.cluster }}
            hidden: ${{inputs.deploy_ray_cluster == false }}
            ignore: ${{ .hidden }}
          kuberay_version:
            label: KubeRay Operator Version
            type: string
            default: 1.3.0
            hidden: ${{inputs.deploy_ray_cluster == false }}
            ignore: ${{ .hidden }}
          ray_image:
            label: KubeRay Operator Version
            type: string
            default: rayproject/ray-ml:2.9.0-gpu
            hidden: ${{inputs.deploy_ray_cluster == false }}
            ignore: ${{ .hidden }}      