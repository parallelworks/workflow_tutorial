permissions:
  - '*'
jobs:
  auth:
    steps:
      - name: Authenticate kubectl
        run: pw kube auth ${{ inputs.k8s.cluster }}
  k8s_deployment:
    needs:
      - auth
    steps:
      - name: Defining job id
        run: |
          job_number=$(pwd | rev | cut -d "/" -f1 | rev)
          echo "export job_number=${job_number}" > job.info
          workflow_name=$(pwd | rev | cut -d "/" -f2 | rev)
          echo "export workflow_name=${workflow_name}" >> job.info
          job_id="${PW_USER}-${workflow_name}-${job_number}-$(date +%s)"
          echo "job_id=${job_id}" | tee -a $OUTPUTS
      - name: Add training script to config map
        env:
          configmap_name: train-script-${{ needs.k8s_deployment.outputs.job_id }}
        run: |
          set -xe
          trap './notify.sh create-configmap 1' ERR
          sed -i "s/__num_epochs__/${{ inputs.model.num_epochs }}/g" train-script.py
          sed -i "s/__lr__/${{ inputs.model.lr }}/g" train-script.py
          sed -i "s/__batch_size__/${{ inputs.model.batch_size }}/g" train-script.py
          sed -i "s|__save_path__|${{ inputs.model.save_path }}|g" train-script.py
          kubectl create configmap ${configmap_name} --from-file=train-script.py -n ${{ inputs.k8s.namespace }}
        cleanup: kubectl delete configmap ${configmap_name} -n ${{ inputs.k8s.namespace }}
      - name: Apply PVC
        env:
          pvc_name: model-storage-${{ needs.k8s_deployment.outputs.job_id }}
        run: |
          set -xe
          trap './notify.sh apply-pvc 2' ERR
          # Create pvc.yaml configuration
          cat > pvc.yaml <<HERE
          apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            name: ${pvc_name}
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage:  ${{ inputs.k8s.pvc_storage }}
          HERE

          # Apply pvc.yaml configuration
          kubectl apply -f pvc.yaml -n ${{ inputs.k8s.namespace }}
        cleanup: kubectl delete -f pvc.yaml -n ${{ inputs.k8s.namespace }}
      - name: Apply training job
        env:
          train_job_name: mnist-train-${{ needs.k8s_deployment.outputs.job_id }}
          pvc_name: model-storage-${{ needs.k8s_deployment.outputs.job_id }}
          configmap_name: train-script-${{ needs.k8s_deployment.outputs.job_id }}
        run: |
          set -xe
          trap './notify.sh apply-train-job 3' ERR
          model_save_path_dir=$(dirname ${{ inputs.model.save_path }})
          # Create train-job.yaml configuration
          cat > train-job.yaml <<HERE
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: ${train_job_name}
          spec:
            template:
              spec:
                containers:
                - name: trainer
                  image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
                  command: ["python", "/scripts/train-script.py"]
                  resources:
                    requests:
                      cpu: "${{ inputs.k8s.cpu_request }}"          # Request 1 CPU core
                      memory: "${{ inputs.k8s.memory_request }}"     # Request 2 GiB of memory
                    limits:
                      nvidia.com/gpu: ${{ inputs.k8s.gpu_limit }}
                  volumeMounts:
                  - mountPath: "${model_save_path_dir}"
                    name: ${pvc_name}
                  - mountPath: "/scripts"
                    name: script-volume
                volumes:
                - name:  ${pvc_name}
                  persistentVolumeClaim:
                    claimName:  ${pvc_name}
                - name: script-volume
                  configMap:
                    name: ${configmap_name}
                restartPolicy: Never
            backoffLimit: 2
          HERE

          # Apply train-job.yaml configuration
          kubectl apply -f train-job.yaml -n ${{ inputs.k8s.namespace }}
        cleanup: kubectl delete -f train-job.yaml -n ${{ inputs.k8s.namespace }}
      - name: Wait for pod to start
        env:
          train_job_name: mnist-train-${{ needs.k8s_deployment.outputs.job_id }}
        run: |

          # Configuration
          NAMESPACE="${{ inputs.k8s.namespace }}"
          TRAIN_JOB_NAME="${train_job_name}"
          MAX_RETRIES=100
          SLEEP_INTERVAL=6

          # Function to get pod phase (returns empty string if no pod exists)
          get_pod_phase() {
            kubectl get pod -n "$NAMESPACE" -l job-name="$TRAIN_JOB_NAME" -o jsonpath="{.items[0].status.phase}" 2>/dev/null || echo ""
          }

          # Function to describe pod for debugging
          describe_pod() {
            local pod_name
            pod_name=$(kubectl get pod -n "$NAMESPACE" -l job-name="$TRAIN_JOB_NAME" -o jsonpath="{.items[0].metadata.name}" 2>/dev/null)
            if [ -n "$pod_name" ]; then
              echo "Pod details:"
              kubectl describe pod "$pod_name" -n "$NAMESPACE"
            else
              echo "No pod found for job $TRAIN_JOB_NAME in namespace $NAMESPACE."
            fi
          }

          echo "Waiting for pod to start for job $TRAIN_JOB_NAME in namespace $NAMESPACE..."
          retries=0

          while [ $retries -lt $MAX_RETRIES ]; do
            phase=$(get_pod_phase)

            # Case 1: No pod exists yet
            if [ -z "$phase" ]; then
              echo "No pod found yet. Waiting... ($retries/$MAX_RETRIES)"
            
            # Case 2: Pod is in Pending (e.g., scheduling or pulling image)
            elif [ "$phase" = "Pending" ]; then
              echo "Pod is in Pending state. Waiting... ($retries/$MAX_RETRIES)"
            
            # Case 3: Pod is Running (success case)
            elif [ "$phase" = "Running" ]; then
              echo "Pod is Running! Proceeding..."
              exit 0
            
            # Case 4: Pod has succeeded (job completed)
            elif [ "$phase" = "Succeeded" ]; then
              echo "Pod has already Succeeded. Job completed."
              exit 0
            
            # Case 5: Pod has failed
            elif [ "$phase" = "Failed" ]; then
              echo "Error: Pod has Failed."
              describe_pod
              ./notify.sh wait-for-pod 4
              exit 1
            
            # Case 6: Unexpected phase
            else
              echo "Unexpected pod phase: $phase. Waiting... ($retries/$MAX_RETRIES)"
            fi

            retries=$((retries + 1))
            if [ $retries -ge $MAX_RETRIES ]; then
              echo "Error: Maximum retries ($MAX_RETRIES) reached! Pod did not start."
              describe_pod
              echo "Job details for further debugging:"
              kubectl describe job "$TRAIN_JOB_NAME" -n "$NAMESPACE"
              ./notify.sh wait-for-pod 4
              exit 1
            fi

            sleep "$SLEEP_INTERVAL"
          done
      - name: Wait for job to finish
        env:
          train_job_name: mnist-train-${{ needs.k8s_deployment.outputs.job_id }}
        run: |
          # Stream logs in background
          kubectl logs -n ${{ inputs.k8s.namespace }} -f job/${train_job_name} &
          LOGS_PID=$!
          # Set trap to kill logs process on exit
          trap 'kill $LOGS_PID 2>/dev/null' EXIT

          echo "Waiting for job ${train_job_name} to finish..."

          # Initialize counter (600s / 5s per iteration = 120 iterations)
          COUNTER=0
          MAX_ITERATIONS=120
          SLEEP_DURATION=5

          # While loop to check job status
          while [ $COUNTER -lt $MAX_ITERATIONS ]; do
              # Check if job is Complete
              if kubectl -n ${{ inputs.k8s.namespace }} get job/${train_job_name} -o jsonpath="{.status.conditions[?(@.type==\"Complete\")].status}" 2>/dev/null | grep -q "True"; then
                  echo "Job ${train_job_name} completed successfully."
                  exit 0
              # Check if job is Failed
              elif kubectl -n ${{ inputs.k8s.namespace }} get job/${train_job_name} -o jsonpath="{.status.conditions[?(@.type==\"Failed\")].status}" 2>/dev/null | grep -q "True"; then
                  echo "Job ${train_job_name} has failed."
                  ./notify.sh wait-for-job 5
                  exit 1
              fi

              # Increment counter and sleep
              COUNTER=$((COUNTER + 1))
              echo "Waiting... ($((COUNTER * SLEEP_DURATION))s elapsed)"
              sleep $SLEEP_DURATION
          done

          # If loop exits, timeout occurred
          echo "Job ${train_job_name} neither completed nor explicitly failed within 10 minutes."
          ./notify.sh wait-for-job 5
          exit 1
      - name: Print logs
        env:
          train_job_name: mnist-train-${{ needs.k8s_deployment.outputs.job_id }}
        run: |
          # Get the pod name associated with the job
          pod_name=$(kubectl get pod -n ${{ inputs.k8s.namespace }} -l job-name=${train_job_name} -o jsonpath="{.items[0].metadata.name}")

          # Check if pod_name is empty (i.e., no pod found)
          if [ -z "$pod_name" ]; then
            echo "Error: No pod found for job ${train_job_name}"
            ./notify.sh print-logs 6
            exit 1
          fi

          # Print the pod name
          echo "Pod name: ${pod_name}"

          # Print the logs of the pod
          kubectl logs ${pod_name} -n ${{ inputs.k8s.namespace }}
        cleanup: |
          # Get the pod name associated with the job
          pod_name=$(kubectl get pod -n ${{ inputs.k8s.namespace }} -l job-name=${train_job_name} -o jsonpath="{.items[0].metadata.name}")

          # Delete the pod if it exists
          if [ -n "$pod_name" ]; then
            echo "Cleaning up pod: ${pod_name}"
            kubectl delete pod ${pod_name} --force --grace-period=0 -n ${{ inputs.k8s.namespace }}
          else
            echo "No pod found to cleanup for job ${train_job_name}"
          fi
      - name: Create transfer pod
        env:
          transfer_pod_name: mnist-transfer-${{ needs.k8s_deployment.outputs.job_id }}
          pvc_name: model-storage-${{ needs.k8s_deployment.outputs.job_id }}
        run: |
          set -xe
          trap './notify.sh apply-transfer-pod 8' ERR
          model_save_path_dir=$(dirname ${{ inputs.model.save_path }})
          # Create transfer-pod.yaml configuration
          cat > transfer-pod.yaml <<HERE
          apiVersion: v1
          kind: Pod
          metadata:
            name: ${transfer_pod_name}
          spec:
            containers:
            - name: transfer
              image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
              command: ["sleep", "3600"]
              resources:
                requests:
                  memory: "100Mi"
                  cpu: "500m"
              volumeMounts:
              - mountPath: "${model_save_path_dir}"
                name: ${pvc_name}
            volumes:
            - name: ${pvc_name}
              persistentVolumeClaim:
                claimName: ${pvc_name}
            restartPolicy: Never
          HERE

          # Apply the transfer pod configuration
          kubectl apply -f transfer-pod.yaml -n ${{ inputs.k8s.namespace }}
        cleanup: kubectl delete pod ${transfer_pod_name} --force --grace-period=0 -n ${{ inputs.k8s.namespace }}
      - name: Transfer ML model
        env:
          transfer_pod_name: mnist-transfer-${{ needs.k8s_deployment.outputs.job_id }}
          pvc_name: model-storage-${{ needs.k8s_deployment.outputs.job_id }}
        run: |
          set -xe
          trap './notify.sh transfer-ml-model 9' ERR
          model_save_path_basename=$(basename ${{ inputs.model.save_path }})
          # Wait for the transfer pod to be running
          echo "Waiting for transfer pod to start..."
          if ! kubectl wait -n ${{ inputs.k8s.namespace }} --for=condition=Ready pod/${transfer_pod_name} --timeout=300s; then
            echo "Error: Transfer pod did not start in time."
            exit 1
          fi

          # Copy the model file from the PVC to the local machine
          if ! kubectl cp ${transfer_pod_name}:${{ inputs.model.save_path }} ./${model_save_path_basename} -n ${{ inputs.k8s.namespace }}; then
            echo "Error: Failed to copy model file."
            exit 1
          fi

          echo "Model file transfer completed successfully."
'on':
  execute:
    inputs:
      k8s:
        type: group
        label: Kubernetes
        items:
          cluster:
            label: Kubernetes cluster
            type: string
            default: minikubegpu2
          namespace:
            label: Namespace
            type: string
            default: alvaro-k8s-testing
          gpu_limit:
            label: GPU Limit
            type: number
            default: 1
            min: 0
            max: 2
          cpu_request:
            label: CPU Request
            type: number
            default: 1
          memory_request:
            label: Memory Request
            type: string
            default: 2Gi
          pvc_storage:
            label: PVC Storage
            type: string
            default: 1Gi
      model:
        type: group
        label: ML Model
        items:
          num_epochs:
            label: Number of Epochs
            type: number
            default: 5
            min: 1
            max: 500
          lr:
            label: Learning Rate
            type: number
            default: 0.01
            min: 0
            max: 1
          batch_size:
            label: Batch Size
            type: number
            default: 64
            min: 8
            max: 128
          save_path:
            label: Save path
            type: string
            default: /mnt/model/mnist_cnn.pt
            tooltip: Path to save the trained model parameters
